# Praktischer RAG Ingestion Guide

## üöÄ Schnellste Bef√ºllung: Step-by-Step

### 1. Vorbereitung (5 Minuten)

```bash
# Ollama Models vorbereiten
ollama pull mistral          # F√ºr semantische Analyse  
ollama pull nomic-embed-text # Backup f√ºr Embeddings

# Python Dependencies
pip install sentence-transformers scikit-learn
```

### 2. Optimale Konfiguration

```python
# F√ºr kleine Wissensbasen (< 1000 Dokumente)
config = IngestionConfig(
    max_workers=4,
    batch_size=32,
    use_local_embedding=True,  # sentence-transformers
    parallel_embedding=True
)

# F√ºr Enterprise (> 10,000 Dokumente)  
config = IngestionConfig(
    max_workers=8,
    batch_size=64,
    chunk_batch_size=200,
    use_local_embedding=True
)
```

### 3. Dokumenten-Verarbeitung Priorit√§ten

#### ‚ö° Sehr Schnell (Keine LLM-Calls)
- **Text-Extraktion**: pypdf, docx, BeautifulSoup
- **Chunking**: RecursiveCharacterTextSplitter  
- **Basis-Metadaten**: Dateiinfo, Wortanzahl, Sprache
- **Embeddings**: sentence-transformers (lokal)

#### üß† Intelligent (Selective LLM-Usage)
- **Keyword-Extraktion**: Nur f√ºr wichtige Dokumente
- **Topic-Klassifikation**: Batch-weise alle 50 Chunks
- **Semantische Verkn√ºpfungen**: Nach Vollindexierung

### 4. Performance-Optimierte Pipeline

```python
async def optimized_pipeline(documents):
    # Phase 1: Bulk Text Processing (Parallel, kein LLM)
    raw_chunks = await extract_all_documents_parallel(documents)
    
    # Phase 2: Bulk Embeddings (lokal, sehr schnell)  
    embeddings = generate_embeddings_batch(raw_chunks)
    
    # Phase 3: Neo4J Bulk Insert (sekunden statt minuten)
    await bulk_insert_neo4j(raw_chunks, embeddings)
    
    # Phase 4: Post-Processing Intelligence (optional)
    await add_semantic_intelligence(smart_analysis=True)
```

## üìà Erwartete Performance

### Realistische Benchmark-Zahlen

| Dokumentenanzahl | Zeit (Hybrid) | Zeit (Nur LLM) | Speedup |
|------------------|---------------|----------------|---------|
| 100 PDFs | 2-5 Minuten | 15-30 Minuten | **6x faster** |
| 1,000 PDFs | 15-25 Minuten | 2-4 Stunden | **8x faster** |
| 10,000 PDFs | 2-3 Stunden | 1-2 Tage | **12x faster** |

### Hardware-Empfehlungen

#### Minimum Setup
- **CPU**: 4 Cores, 16GB RAM
- **Performance**: ~10 Dokumente/Minute
- **Geeignet f√ºr**: Kleine Teams, Prototyping

#### Optimaler Setup  
- **CPU**: 8+ Cores, 32GB RAM
- **GPU**: Optional (f√ºr bessere Embeddings)
- **Performance**: ~50-100 Dokumente/Minute
- **Geeignet f√ºr**: Enterprise, gro√üe Datenmengen

#### High-End Setup
- **CPU**: 16+ Cores, 64GB RAM  
- **GPU**: RTX 4090 oder Tesla
- **Performance**: ~200+ Dokumente/Minute
- **Geeignet f√ºr**: Massive Datenmengen

## üéõÔ∏è Adaptive Strategien

### Smart Processing Rules

1. **Dokumentgr√∂√üe < 1MB**: Vollst√§ndige LLM-Analyse
2. **Dokumentgr√∂√üe > 1MB**: Sampling + Basis-Analyse  
3. **Bekannte Formate** (PDF, DOCX): Optimierte Extraktion
4. **Unbekannte Formate**: Fallback auf Text-Extraktion

### Quality vs Speed Trade-offs

```python
# Maximum Speed (90% Qualit√§t)
SPEED_MODE = {
    'llm_analysis': False,
    'simple_keywords': True,
    'batch_embeddings': True,
    'parallel_workers': 8
}

# Balanced (95% Qualit√§t)
BALANCED_MODE = {
    'llm_analysis': 'selective',  # Nur f√ºr wichtige Docs
    'smart_keywords': True,
    'semantic_connections': 'post_process'
}

# Maximum Quality (100% Qualit√§t)
QUALITY_MODE = {
    'llm_analysis': True,
    'deep_semantic': True,
    'cross_document_analysis': True,
    'processing_time': '3x longer'
}
```

## üîß Troubleshooting

### H√§ufige Probleme & L√∂sungen

#### Problem: "Ollama zu langsam f√ºr Embeddings"
```python
# L√∂sung: Verwende sentence-transformers
config.use_local_embedding = True
# 10x Performance-Boost!
```

#### Problem: "Memory-Probleme bei gro√üen Batches"  
```python
# L√∂sung: Kleinere Batches + Cleanup
config.batch_size = 16
config.chunk_batch_size = 50
# + Memory cleanup zwischen Batches
```

#### Problem: "Neo4J Bulk Insert langsam"
```python
# L√∂sung: Optimierte Queries + Batching
BULK_INSERT_BATCH_SIZE = 1000
# + APOC Procedures aktivieren
```

#### Problem: "CPU-Bottleneck"
```python  
# L√∂sung: ProcessPoolExecutor statt ThreadPool
with ProcessPoolExecutor(max_workers=cpu_count()) as executor:
    # CPU-intensive Tasks parallel
```

## üìä Monitoring & Optimization

### Performance-Metriken tracken

```python
metrics = {
    'docs_per_second': 15.2,
    'chunks_per_second': 245.8,
    'embedding_generation_time': 0.05,  # pro Chunk
    'memory_usage_peak': '8.2 GB',
    'cpu_utilization_avg': '75%'
}
```

### Adaptive Optimierung

```python
# System passt sich automatisch an
if metrics['docs_per_second'] < 10:
    config.batch_size *= 2  # Gr√∂√üere Batches
    
if metrics['memory_usage_peak'] > '16GB':
    config.max_workers = max(2, config.max_workers - 1)
```

## üéØ Empfohlenes Setup f√ºr verschiedene Szenarien

### üìö Academic Research (1K-10K papers)
```python
ACADEMIC_CONFIG = IngestionConfig(
    batch_size=32,
    use_local_embedding=True,
    analysis_model="mistral",
    focus_on='citations_and_topics'
)
# Expected: 3-5 hours f√ºr 10K Papers
```

### üè¢ Enterprise Knowledge Base (10K-100K docs)
```python
ENTERPRISE_CONFIG = IngestionConfig(
    max_workers=8,
    batch_size=64,
    chunk_batch_size=200,
    parallel_embedding=True,
    smart_deduplication=True
)
# Expected: 8-12 hours f√ºr 100K Dokumente
```

### üöÄ Startup MVP (100-1K docs)
```python
STARTUP_CONFIG = IngestionConfig(
    max_workers=4,
    batch_size=16,
    use_local_embedding=True,
    minimal_analysis=True
)  
# Expected: 30-60 Minuten f√ºr 1K Dokumente
```

## üí° Pro-Tips

1. **Starte mit Speed-Mode** ‚Üí Optimiere sp√§ter f√ºr Qualit√§t
2. **Dokumenten-Sampling** ‚Üí Teste Pipeline mit 100 Docs first
3. **Incremental Processing** ‚Üí Neue Docs automatisch hinzuf√ºgen  
4. **Format-Specific Optimization** ‚Üí Verschiedene Pipelines f√ºr PDF vs Web
5. **Error Recovery** ‚Üí System recovered gracefully von failed docs
6. **Progress Monitoring** ‚Üí Live-Dashboard f√ºr lange Ingestions

## üé™ One-Liner f√ºr Quick Start

```bash
# Complete setup und erste 1000 Dokumente in ~30 Minuten
python -c "
from ingestion import HighPerformanceDocumentIngester, IngestionConfig
import asyncio

config = IngestionConfig(use_local_embedding=True)
ingester = HighPerformanceDocumentIngester(config)
asyncio.run(ingester.ingest_documents_optimized(['./documents/'], rag_system))
"
```

# Highly Efficient RAG Document Ingestion Pipeline
# Optimiert f√ºr lokale LLMs mit intelligenter Parallelisierung
```python
import asyncio
import aiofiles
import concurrent.futures
from concurrent.futures import ProcessPoolExecutor, ThreadPoolExecutor
from typing import List, Dict, Any, Optional, Tuple
from dataclasses import dataclass
import time
from pathlib import Path
import hashlib
import json
import logging
from queue import Queue
import multiprocessing as mp
import psutil
```
# Core processing imports
```python
import pypdf
from bs4 import BeautifulSoup
import docx
import pandas as pd
from PIL import Image
import pytesseract
import requests
import asyncio
from urllib.parse import urljoin, urlparse
import aiohttp
```
# ML/AI imports  
```python
from sentence_transformers import SentenceTransformer
import numpy as np
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.llms import Ollama

@dataclass
class IngestionConfig:
    """Konfiguration f√ºr optimale Document Ingestion"""
    
    # Performance Settings
    max_workers: int = min(8, mp.cpu_count())
    batch_size: int = 32  # Dokumente pro Batch
    chunk_batch_size: int = 100  # Chunks pro Embedding-Batch
    
    # LLM Settings
    ollama_base_url: str = "http://localhost:11434"
    analysis_model: str = "mistral"  # Schneller f√ºr Analyse
    embedding_model: str = "nomic-embed-text"
    
    # Document Processing
    max_file_size_mb: int = 100
    supported_formats: List[str] = None
    
    # Optimization
    use_local_embedding: bool = True  # sentence-transformers f√ºr Speed
    parallel_embedding: bool = True
    cache_results: bool = True
    skip_existing: bool = True
    
    # Quality Control
    min_chunk_length: int = 50
    max_chunk_length: int = 1500
    chunk_overlap: int = 200
    
    def __post_init__(self):
        if self.supported_formats is None:
            self.supported_formats = ['.pdf', '.docx', '.txt', '.html', '.md', '.csv', '.xlsx']

class HighPerformanceDocumentIngester:
    """
    Hochperformante Dokument-Ingestion mit intelligenter Parallelisierung
    """
    
    def __init__(self, config: IngestionConfig):
        self.config = config
        self.processed_hashes = set()
        self.setup_logging()
        
        # Initialize components
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.config.max_chunk_length,
            chunk_overlap=self.config.chunk_overlap
        )
        
        # Local embedding model (viel schneller als Ollama f√ºr Embeddings)
        if self.config.use_local_embedding:
            self.embedding_model = SentenceTransformer('sentence-transformers/all-MiniLM-L6-v2')
        
        # LLM f√ºr semantische Analyse (nur wo n√∂tig)
        self.analysis_llm = Ollama(
            base_url=self.config.ollama_base_url,
            model=self.config.analysis_model
        )
        
        # Performance tracking
        self.stats = {
            'documents_processed': 0,
            'chunks_created': 0,
            'embeddings_generated': 0,
            'processing_time': 0,
            'errors': []
        }
        
        # Load existing document hashes for deduplication
        self.load_processed_hashes()
    
    def setup_logging(self):
        """Setup optimized logging"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('ingestion.log'),
                logging.StreamHandler()
            ]
        )
        self.logger = logging.getLogger(__name__)
    
    async def ingest_documents_optimized(self, sources: List[str], 
                                       rag_system: Any) -> Dict[str, Any]:
        """
        Hauptfunktion f√ºr optimierte Dokumenten-Ingestion
        """
        
        start_time = time.time()
        self.logger.info(f"üöÄ Starting optimized ingestion of {len(sources)} sources")
        
        # 1. Parallele Dokumentenerkennung und -filterung
        valid_sources = await self._discover_and_filter_documents(sources)
        self.logger.info(f"üìÑ Found {len(valid_sources)} valid documents")
        
        # 2. Batch-weise Verarbeitung f√ºr optimale Resource-Nutzung  
        document_batches = self._create_batches(valid_sources, self.config.batch_size)
        
        all_processed_docs = []
        
        for batch_idx, batch in enumerate(document_batches):
            self.logger.info(f"üì¶ Processing batch {batch_idx + 1}/{len(document_batches)}")
            
            # Parallele Dokumentenverarbeitung
            batch_results = await self._process_document_batch(batch)
            all_processed_docs.extend(batch_results)
            
            # Memory cleanup zwischen Batches
            if batch_idx % 5 == 0:
                await asyncio.sleep(0.1)  # Kurze Pause f√ºr GC
        
        # 3. Hocheffiziente Embedding-Generierung
        all_chunks = []
        for doc_result in all_processed_docs:
            all_chunks.extend(doc_result['chunks'])
        
        self.logger.info(f"üî¢ Generating embeddings for {len(all_chunks)} chunks")
        embeddings_results = await self._generate_embeddings_batch(all_chunks)
        
        # 4. Intelligente Graph-Integration
        self.logger.info(f"üï∏Ô∏è Building knowledge graph connections")
        await self._build_intelligent_connections(all_processed_docs, rag_system)
        
        # 5. Neo4J Bulk Insert (deutlich effizienter)
        await self._bulk_insert_to_neo4j(all_processed_docs, embeddings_results, rag_system)
        
        # Final statistics
        total_time = time.time() - start_time
        self.stats['processing_time'] = total_time
        
        final_stats = {
            'total_documents': len(all_processed_docs),
            'total_chunks': len(all_chunks),
            'processing_time_seconds': total_time,
            'documents_per_second': len(all_processed_docs) / total_time,
            'chunks_per_second': len(all_chunks) / total_time,
            'performance_metrics': self._calculate_performance_metrics(),
            'errors': self.stats['errors']
        }
        
        self.logger.info(f"‚úÖ Ingestion completed in {total_time:.2f}s")
        self.logger.info(f"üìä Performance: {final_stats['documents_per_second']:.2f} docs/sec")
        
        return final_stats
    
    async def _discover_and_filter_documents(self, sources: List[str]) -> List[Dict[str, Any]]:
        """Intelligente Dokumentenerkennung und Filterung"""
        
        discovered_docs = []
        
        # Parallele Verarbeitung verschiedener Source-Typen
        tasks = []
        
        for source in sources:
            if Path(source).is_file():
                tasks.append(self._process_file_source(source))
            elif Path(source).is_dir():
                tasks.append(self._process_directory_source(source))
            elif source.startswith('http'):
                tasks.append(self._process_url_source(source))
            else:
                self.logger.warning(f"‚ö†Ô∏è Unknown source type: {source}")
        
        # Warte auf alle Discovery-Tasks
        if tasks:
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            for result in results:
                if isinstance(result, Exception):
                    self.logger.error(f"‚ùå Discovery error: {result}")
                    self.stats['errors'].append(str(result))
                else:
                    discovered_docs.extend(result)
        
        # Deduplication basierend auf Content-Hash
        unique_docs = self._deduplicate_documents(discovered_docs)
        
        return unique_docs
    
    async def _process_file_source(self, file_path: str) -> List[Dict[str, Any]]:
        """Verarbeitet einzelne Dateien"""
        
        path = Path(file_path)
        if not path.exists():
            return []
        
        # File validation
        if path.stat().st_size > self.config.max_file_size_mb * 1024 * 1024:
            self.logger.warning(f"‚ö†Ô∏è File too large: {file_path}")
            return []
        
        if path.suffix.lower() not in self.config.supported_formats:
            self.logger.warning(f"‚ö†Ô∏è Unsupported format: {file_path}")
            return []
        
        return [{
            'path': str(path),
            'type': 'file',
            'format': path.suffix.lower(),
            'size': path.stat().st_size,
            'modified': path.stat().st_mtime
        }]
    
    async def _process_directory_source(self, dir_path: str) -> List[Dict[str, Any]]:
        """Verarbeitet Verzeichnisse rekursiv"""
        
        docs = []
        path = Path(dir_path)
        
        if not path.exists():
            return docs
        
        # Rekursive Dateisuche mit async
        for file_path in path.rglob('*'):
            if file_path.is_file():
                file_docs = await self._process_file_source(str(file_path))
                docs.extend(file_docs)
        
        return docs
    
    async def _process_url_source(self, url: str) -> List[Dict[str, Any]]:
        """Verarbeitet Web-URLs"""
        
        try:
            async with aiohttp.ClientSession() as session:
                async with session.get(url) as response:
                    if response.status == 200:
                        content_type = response.headers.get('content-type', '')
                        
                        return [{
                            'path': url,
                            'type': 'url',
                            'format': self._detect_format_from_content_type(content_type),
                            'size': len(await response.read()),
                            'content_type': content_type
                        }]
        except Exception as e:
            self.logger.error(f"‚ùå URL processing failed {url}: {e}")
            self.stats['errors'].append(f"URL error {url}: {e}")
        
        return []
    
    def _detect_format_from_content_type(self, content_type: str) -> str:
        """Erkennt Format aus Content-Type"""
        
        mappings = {
            'application/pdf': '.pdf',
            'text/html': '.html',
            'text/plain': '.txt',
            'application/json': '.json'
        }
        
        for ct, fmt in mappings.items():
            if ct in content_type:
                return fmt
        
        return '.html'  # Default f√ºr Web-Content
    
    def _deduplicate_documents(self, docs: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Deduplizierung basierend auf Content-Hash"""
        
        unique_docs = []
        seen_hashes = set()
        
        for doc in docs:
            # Einfacher Hash f√ºr Dateipfad + Gr√∂√üe + Datum
            content_id = f"{doc['path']}_{doc['size']}_{doc.get('modified', 0)}"
            content_hash = hashlib.md5(content_id.encode()).hexdigest()
            
            if content_hash not in seen_hashes and content_hash not in self.processed_hashes:
                unique_docs.append(doc)
                seen_hashes.add(content_hash)
                doc['content_hash'] = content_hash
            else:
                self.logger.info(f"‚è≠Ô∏è Skipping duplicate/existing: {doc['path']}")
        
        return unique_docs
    
    async def _process_document_batch(self, batch: List[Dict[str, Any]]) -> List[Dict[str, Any]]:
        """Verarbeitet einen Batch von Dokumenten parallel"""
        
        # ProcessPoolExecutor f√ºr CPU-intensive Text-Extraktion
        with ProcessPoolExecutor(max_workers=self.config.max_workers) as executor:
            
            # Tasks f√ºr parallele Verarbeitung erstellen
            loop = asyncio.get_event_loop()
            tasks = []
            
            for doc in batch:
                task = loop.run_in_executor(
                    executor, 
                    self._extract_and_process_document, 
                    doc
                )
                tasks.append(task)
            
            # Warten auf alle Tasks
            results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Fehlerbehandlung
            valid_results = []
            for i, result in enumerate(results):
                if isinstance(result, Exception):
                    self.logger.error(f"‚ùå Document processing error {batch[i]['path']}: {result}")
                    self.stats['errors'].append(f"Processing error {batch[i]['path']}: {result}")
                else:
                    valid_results.append(result)
                    self.stats['documents_processed'] += 1
            
            return valid_results
    
    def _extract_and_process_document(self, doc: Dict[str, Any]) -> Dict[str, Any]:
        """Extrahiert und verarbeitet ein einzelnes Dokument (CPU-intensiv)"""
        
        try:
            # Text-Extraktion basierend auf Format
            if doc['format'] == '.pdf':
                text = self._extract_pdf_text(doc['path'])
            elif doc['format'] == '.docx':
                text = self._extract_docx_text(doc['path'])
            elif doc['format'] == '.html':
                text = self._extract_html_text(doc['path'])
            elif doc['format'] in ['.txt', '.md']:
                text = self._extract_plain_text(doc['path'])
            elif doc['format'] in ['.csv', '.xlsx']:
                text = self._extract_tabular_text(doc['path'])
            else:
                text = self._extract_fallback_text(doc['path'])
            
            # Qualit√§tspr√ºfung
            if len(text.strip()) < self.config.min_chunk_length:
                self.logger.warning(f"‚ö†Ô∏è Document too short: {doc['path']}")
                return None
            
            # Text-Chunking
            chunks = self.text_splitter.split_text(text)
            
            # Chunk-Qualit√§tspr√ºfung und Bereinigung
            quality_chunks = []
            for chunk in chunks:
                cleaned_chunk = self._clean_chunk_text(chunk)
                if len(cleaned_chunk.strip()) >= self.config.min_chunk_length:
                    quality_chunks.append({
                        'content': cleaned_chunk,
                        'source': doc['path'],
                        'chunk_index': len(quality_chunks),
                        'format': doc['format']
                    })
            
            self.stats['chunks_created'] += len(quality_chunks)
            
            # Dokumenten-Metadaten extrahieren
            metadata = self._extract_document_metadata(text, doc)
            
            return {
                'document': doc,
                'metadata': metadata,
                'chunks': quality_chunks,
                'text_length': len(text),
                'chunk_count': len(quality_chunks)
            }
            
        except Exception as e:
            self.logger.error(f"‚ùå Document extraction failed {doc['path']}: {e}")
            raise e
    
    def _extract_pdf_text(self, path: str) -> str:
        """Optimierte PDF-Text-Extraktion"""
        
        if Path(path).exists():
            with open(path, 'rb') as file:
                reader = pypdf.PdfReader(file)
                text_parts = []
                
                for page in reader.pages:
                    text_parts.append(page.extract_text())
                
                return '\n'.join(text_parts)
        else:
            # URL-basierte PDF
            response = requests.get(path)
            reader = pypdf.PdfReader(response.content)
            return '\n'.join(page.extract_text() for page in reader.pages)
    
    def _extract_docx_text(self, path: str) -> str:
        """Word-Dokument Extraktion"""
        
        doc = docx.Document(path)
        return '\n'.join(paragraph.text for paragraph in doc.paragraphs)
    
    def _extract_html_text(self, path: str) -> str:
        """HTML-Text-Extraktion"""
        
        if path.startswith('http'):
            response = requests.get(path)
            html_content = response.text
        else:
            with open(path, 'r', encoding='utf-8') as f:
                html_content = f.read()
        
        soup = BeautifulSoup(html_content, 'html.parser')
        
        # Remove script and style elements
        for script in soup(["script", "style"]):
            script.decompose()
        
        return soup.get_text(separator='\n', strip=True)
    
    def _extract_plain_text(self, path: str) -> str:
        """Plain Text Extraktion"""
        
        with open(path, 'r', encoding='utf-8') as f:
            return f.read()
    
    def _extract_tabular_text(self, path: str) -> str:
        """CSV/Excel Extraktion"""
        
        if path.endswith('.csv'):
            df = pd.read_csv(path)
        else:
            df = pd.read_excel(path)
        
        # Convert to structured text
        text_parts = [f"Data from {Path(path).name}:"]
        
        # Column headers
        text_parts.append("Columns: " + ", ".join(df.columns))
        
        # Sample data (erste 10 Zeilen)
        for _, row in df.head(10).iterrows():
            row_text = " | ".join(str(value) for value in row.values)
            text_parts.append(row_text)
        
        # Summary statistics
        text_parts.append(f"\nDataset contains {len(df)} rows and {len(df.columns)} columns")
        
        return '\n'.join(text_parts)
    
    def _extract_fallback_text(self, path: str) -> str:
        """Fallback f√ºr unbekannte Formate"""
        
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except UnicodeDecodeError:
            with open(path, 'r', encoding='latin-1') as f:
                return f.read()
    
    def _clean_chunk_text(self, chunk: str) -> str:
        """Bereinigt Chunk-Text f√ºr bessere Qualit√§t"""
        
        # Entferne excessive whitespace
        chunk = ' '.join(chunk.split())
        
        # Entferne sehr kurze "Junk" Zeilen
        lines = chunk.split('\n')
        clean_lines = [line for line in lines if len(line.strip()) > 10 or line.strip().endswith('.')]
        
        return '\n'.join(clean_lines)
    
    def _extract_document_metadata(self, text: str, doc: Dict[str, Any]) -> Dict[str, Any]:
        """Extrahiert Metadaten ohne LLM (f√ºr Speed)"""
        
        # Basis-Metadaten
        metadata = {
            'source': doc['path'],
            'format': doc['format'],
            'size': doc['size'],
            'char_count': len(text),
            'word_count': len(text.split()),
            'estimated_reading_time_min': len(text.split()) / 200  # 200 WPM
        }
        
        # Einfache Spracherkennung (ohne LLM)
        if any(word in text.lower() for word in ['the', 'and', 'is', 'in', 'to']):
            metadata['language'] = 'english'
        elif any(word in text.lower() for word in ['der', 'die', 'das', 'und', 'ist']):
            metadata['language'] = 'german'
        else:
            metadata['language'] = 'unknown'
        
        return metadata
    
    async def _generate_embeddings_batch(self, chunks: List[Dict[str, Any]]) -> Dict[str, np.ndarray]:
        """Hocheffiziente Batch-Embedding-Generierung"""
        
        if not chunks:
            return {}
        
        self.logger.info(f"üî¢ Generating embeddings for {len(chunks)} chunks")
        
        # Erstelle Batches f√ºr Embedding
        chunk_batches = self._create_batches(chunks, self.config.chunk_batch_size)
        all_embeddings = {}
        
        for batch_idx, chunk_batch in enumerate(chunk_batches):
            self.logger.info(f"üìä Embedding batch {batch_idx + 1}/{len(chunk_batches)}")
            
            # Extrahiere Texte
            texts = [chunk['content'] for chunk in chunk_batch]
            
            if self.config.use_local_embedding:
                # Lokale Embeddings (viel schneller)
                embeddings = self.embedding_model.encode(
                    texts, 
                    batch_size=32, 
                    show_progress_bar=False,
                    normalize_embeddings=True
                )
            else:
                # Ollama Embeddings (langsamer aber eventuell bessere Qualit√§t)
                embeddings = await self._generate_ollama_embeddings(texts)
            
            # Speichere Embeddings mit Chunk-IDs
            for i, chunk in enumerate(chunk_batch):
                chunk_id = f"{chunk['source']}_{chunk['chunk_index']}"
                all_embeddings[chunk_id] = embeddings[i]
            
            self.stats['embeddings_generated'] += len(chunk_batch)
        
        return all_embeddings
    
    async def _generate_ollama_embeddings(self, texts: List[str]) -> np.ndarray:
        """Generiert Embeddings √ºber Ollama (falls gew√ºnscht)"""
        
        # Vereinfachte Implementierung - in der Praxis w√ºrde man hier
        # die Ollama REST API f√ºr Embeddings verwenden
        embeddings = []
        
        for text in texts:
            # Placeholder - ersetze mit echtem Ollama Embedding Call
            embedding = np.random.random(384)  # nomic-embed-text Dimension
            embeddings.append(embedding)
        
        return np.array(embeddings)
    
    async def _build_intelligent_connections(self, processed_docs: List[Dict[str, Any]], 
                                           rag_system: Any) -> None:
        """Erstellt intelligente Verbindungen zwischen Dokumenten"""
        
        self.logger.info("üï∏Ô∏è Building intelligent document connections")
        
        # Sammle alle Chunks f√ºr Cross-Document-Analyse
        all_chunks = []
        chunk_doc_mapping = {}
        
        for doc_result in processed_docs:
            for chunk in doc_result['chunks']:
                chunk_id = f"{chunk['source']}_{chunk['chunk_index']}"
                all_chunks.append(chunk['content'])
                chunk_doc_mapping[chunk_id] = doc_result['document']['path']
        
        # Nur bei ausreichend Chunks Cross-Document-Analyse machen
        if len(all_chunks) > 10:
            await self._create_semantic_connections(all_chunks, chunk_doc_mapping, rag_system)
    
    async def _create_semantic_connections(self, chunks: List[str], 
                                         mapping: Dict[str, str], rag_system: Any) -> None:
        """Erstellt semantische Verbindungen zwischen Chunks"""
        
        # Vereinfachte semantische Analyse ohne schwere LLM-Calls
        # In der Praxis w√ºrde man hier Similarity-Matrices verwenden
        
        self.logger.info(f"üîó Creating semantic connections for {len(chunks)} chunks")
        
        # Erstelle einfache Keyword-basierte Verbindungen
        # (f√ºr echte semantische Analyse w√ºrde man Embedding-Similarity verwenden)
        
        common_terms = self._extract_common_terms(chunks)
        
        with rag_system.driver.session() as session:
            for term in common_terms[:50]:  # Limitiere auf Top 50 Terms
                session.run("""
                    MERGE (k:Keyword {term: $term})
                """, {'term': term.lower()})
    
    def _extract_common_terms(self, chunks: List[str]) -> List[str]:
        """Extrahiert h√§ufige Begriffe ohne LLM"""
        
        from collections import Counter
        import re
        
        # Einfache Term-Extraktion
        all_words = []
        for chunk in chunks:
            # Extrahiere W√∂rter (vereinfacht)
            words = re.findall(r'\b[a-zA-Z]{4,}\b', chunk.lower())
            all_words.extend(words)
        
        # H√§ufigste Begriffe
        common_words = Counter(all_words).most_common(100)
        
        # Filtere Stopwords (vereinfacht)
        stopwords = {'this', 'that', 'with', 'have', 'will', 'from', 'they', 'been', 'than', 'were'}
        
        return [word for word, count in common_words if word not in stopwords and count > 2]
    
    async def _bulk_insert_to_neo4j(self, processed_docs: List[Dict[str, Any]], 
                                   embeddings: Dict[str, np.ndarray], rag_system: Any) -> None:
        """Bulk-Insert in Neo4J f√ºr maximale Performance"""
        
        self.logger.info("üíæ Bulk inserting into Neo4J")
        
        with rag_system.driver.session() as session:
            # Vorbereitung f√ºr Bulk Insert
            chunk_data = []
            
            for doc_result in processed_docs:
                for chunk in doc_result['chunks']:
                    chunk_id = f"{chunk['source']}_{chunk['chunk_index']}"
                    embedding = embeddings.get(chunk_id)
                    
                    chunk_data.append({
                        'id': chunk_id,
                        'content': chunk['content'],
                        'source': chunk['source'],
                        'chunk_index': chunk['chunk_index'],
                        'format': chunk['format'],
                        'embedding': embedding.tolist() if embedding is not None else None
                    })
            
            # Bulk Insert Query
            bulk_query = """
            UNWIND $chunk_data as chunk
            MERGE (c:Chunk {id: chunk.id})
            SET c.content = chunk.content,
                c.source = chunk.source,
                c.chunk_index = chunk.chunk_index,
                c.format = chunk.format,
                c.embedding = chunk.embedding,
                c.created = datetime(),
                c.length = size(chunk.content)
            """
            
            # F√ºhre Bulk Insert in Batches aus (f√ºr sehr gro√üe Datasets)
            batch_size = 1000
            for i in range(0, len(chunk_data), batch_size):
                batch = chunk_data[i:i + batch_size]
                session.run(bulk_query, {'chunk_data': batch})
                
                self.logger.info(f"üíæ Inserted batch {i//batch_size + 1}/{(len(chunk_data) + batch_size - 1)//batch_size}")
        
        self.logger.info(f"‚úÖ Successfully inserted {len(chunk_data)} chunks into Neo4J")
    
    def _create_batches(self, items: List[Any], batch_size: int) -> List[List[Any]]:
        """Erstellt Batches aus Liste"""
        
        batches = []
        for i in range(0, len(items), batch_size):
            batches.append(items[i:i + batch_size])
        return batches
    
    def _calculate_performance_metrics(self) -> Dict[str, Any]:
        """Berechnet Performance-Metriken"""
        
        total_time = self.stats['processing_time']
        
        return {
            'documents_per_second': self.stats['documents_processed'] / total_time if total_time > 0 else 0,
            'chunks_per_second': self.stats['chunks_created'] / total_time if total_time > 0 else 0,
            'embeddings_per_second': self.stats['embeddings_generated'] / total_time if total_time > 0 else 0,
            'average_chunks_per_document': self.stats['chunks_created'] / max(self.stats['documents_processed'], 1),
            'error_rate': len(self.stats['errors']) / max(self.stats['documents_processed'], 1),
            'cpu_usage': psutil.cpu_percent(),
            'memory_usage_gb': psutil.virtual_memory().used / (1024**3)
        }
    
    def load_processed_hashes(self):
        """L√§dt bereits verarbeitete Dokument-Hashes"""
        
        hash_file = Path("processed_documents.json")
        if hash_file.exists():
            try:
                with open(hash_file, 'r') as f:
                    data = json.load(f)
                    self.processed_hashes = set(data.get('processed_hashes', []))
                self.logger.info(f"üìÅ Loaded {len(self.processed_hashes)} processed document hashes")
            except Exception as e:
                self.logger.warning(f"‚ö†Ô∏è Could not load processed hashes: {e}")
    
    def save_processed_hashes(self):
        """Speichert verarbeitete Dokument-Hashes"""
        
        hash_file = Path("processed_documents.json")
        try:
            with open(hash_file, 'w') as f:
                json.dump({
                    'processed_hashes': list(self.processed_hashes),
                    'last_updated': time.time()
                }, f)
            self.logger.info(f"üíæ Saved {len(self.processed_hashes)} processed document hashes")
        except Exception as e:
            self.logger.error(f"‚ùå Could not save processed hashes: {e}")


# Usage Example & Performance Test
async def test_high_performance_ingestion():
    """Test der High-Performance Ingestion"""
    
    # Setup
    config = IngestionConfig(
        max_workers=4,
        batch_size=16,
        chunk_batch_size=50,
        use_local_embedding=True  # Deutlich schneller!
    )
    
    ingester = HighPerformanceDocumentIngester(config)
    
    # Mock RAG System (ersetze mit echtem System)
    class MockRAGSystem:
        def __init__(self):
            import neo4j
            self.driver = neo4j.GraphDatabase.driver("bolt://localhost:7687", 
                                                    auth=("neo4j", "password"))
    
    rag_system = MockRAGSystem()
    
    # Test-Dokumente
    test_sources = [
        "documents/",  # Verzeichnis mit PDFs
        "https://example.com/article.html",  # Web-URL
        "data.csv"  # Strukturierte Daten
    ]
    
    # Performance Test
    print("üöÄ Starting High-Performance Document Ingestion Test")
    start_time = time.time()
    
    results = await ingester.ingest_documents_optimized(test_sources, rag_system)
    
    total_time = time.time() - start_time
    
    print(f"\nüìä PERFORMANCE RESULTS:")
    print(f"Total Time: {total_time:.2f} seconds")
    print(f"Documents Processed: {results['total_documents']}")
    print(f"Chunks Created: {results['total_chunks']}")
    print(f"Processing Speed: {results['documents_per_second']:.2f} docs/sec")
    print(f"Chunk Speed: {results['chunks_per_second']:.2f} chunks/sec")
    
    # Save processed hashes
    ingester.save_processed_hashes()

if __name__ == "__main__":
    asyncio.run(test_high_performance_ingestion())
```