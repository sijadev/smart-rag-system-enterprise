"""
Intelligent Data Import Workflow f√ºr Smart RAG System
===================================================

Implementiert einen automatischen Workflow, bei dem:
1. Entity-Analyzer-LLM fehlende Daten erkennt
2. Chat-LLM konsultiert wird, um Wissensl√ºcken zu f√ºllen
3. Neue Daten automatisch in die Wissensdatenbank importiert werden
4. System sich selbst erweitert und verbessert
"""

import asyncio
import hashlib
import json
from dataclasses import dataclass
from datetime import datetime
from pathlib import Path
from typing import Any, Dict, List, Optional

from src.rag_system import AdvancedRAGSystem, RAGConfig
from src.self_learning_rag import SelfLearningRAGSystem

# Import the enterprise workarounds
try:
    from enterprise_workarounds import enhance_rag_with_workarounds

    WORKAROUNDS_AVAILABLE = True
except ImportError:
    WORKAROUNDS_AVAILABLE = False
    print("‚ö†Ô∏è Enterprise workarounds not available")


@dataclass
class DataGapAnalysis:
    """Analyse von Wissensl√ºcken in der Datenbank"""

    query: str
    missing_concepts: List[str]
    confidence_score: float
    suggested_queries: List[str]
    data_sources: List[str]
    timestamp: datetime


class IntelligentDataImportWorkflow:
    """
    Intelligenter Workflow f√ºr automatischen Datenimport bei erkannten Wissensl√ºcken
    """

    def __init__(self, rag_system: AdvancedRAGSystem):
        self.rag_system = rag_system
        self.data_import_history: List[DataGapAnalysis] = []
        self.auto_import_path = Path("data/auto_imported")
        self.auto_import_path.mkdir(exist_ok=True)

        # Separate LLM-Instanzen fÔøΩÔøΩr verschiedene Aufgaben
        # F√ºr Gap-Analyse (braucht JSON-Strukturierung)
        self.gap_analyzer = rag_system.chat_llm
        # F√ºr Wissensgenerierung (braucht detaillierte Inhalte)
        self.knowledge_generator = rag_system.entity_analyzer_llm

        # Konfiguration
        self.min_confidence_threshold = 0.7
        self.max_auto_imports_per_session = 5
        self.current_imports = 0

    async def analyze_knowledge_gaps(
        self, query: str, current_answer: str, context_quality: float
    ) -> Optional[DataGapAnalysis]:
        """
        Analysiert ob die aktuelle Antwort Wissensl√ºcken aufweist
        """
        # Versch√§rfte Bedingungen f√ºr Gap-Analyse
        if context_quality > 0.6:  # Reduziert von 0.8 auf 0.6
            print(
                f"‚úÖ Answer quality {context_quality:.2f} is good enough - no gap analysis needed"
            )
            return None

        print(f"üîç Starting gap analysis - quality score: {context_quality:.2f}")

        # Spezielle Behandlung f√ºr sehr schlechte Qualit√§t (keine Quellen)
        if context_quality <= 0.2:
            print("üö® Very poor quality detected - forcing gap analysis")
            # Extrahiere Hauptkonzepte aus der Query f√ºr Auto-Import
            query_concepts = self._extract_query_concepts(query)

            return DataGapAnalysis(
                query=query,
                missing_concepts=query_concepts,
                confidence_score=0.9,  # Hohe Konfidenz bei sehr schlechter Qualit√§t
                suggested_queries=[
                    f"What is {concept}?" for concept in query_concepts[:3]
                ],
                data_sources=["knowledge_base", "expert_knowledge"],
                timestamp=datetime.now(),
            )

        gap_analysis_prompt = f"""
        Analyze this query and response to identify knowledge gaps:

        Query: {query}
        Current Response: {current_answer}
        Context Quality Score: {context_quality}

        The response quality is poor (score: {context_quality:.2f}). Identify missing concepts.
        Return a JSON response with:
        {{
            "has_gaps": true,
            "missing_concepts": ["concept1", "concept2"],
            "confidence": 0.8,
            "suggested_queries": ["query1", "query2"],
            "data_sources": ["source1", "source2"]
        }}

        Focus on the main topics in the query that need more information.
        """

        try:
            response = await self.gap_analyzer.agenerate([gap_analysis_prompt])
            gap_data = self._parse_json_response(response.generations[0][0].text)

            # Reduzierter Schwellenwert f√ºr bessere Auto-Import-Ausl√∂sung
            confidence_threshold = 0.5  # Reduziert von 0.7

            if (
                gap_data.get("has_gaps", False)
                and gap_data.get("confidence", 0) > confidence_threshold
            ):
                print(
                    f"‚úÖ Gap analysis successful - confidence: {gap_data.get('confidence', 0):.2f}"
                )
                return DataGapAnalysis(
                    query=query,
                    missing_concepts=gap_data.get("missing_concepts", []),
                    confidence_score=gap_data.get("confidence", 0.8),
                    suggested_queries=gap_data.get("suggested_queries", []),
                    data_sources=gap_data.get("data_sources", []),
                    timestamp=datetime.now(),
                )
            else:
                print(
                    f"‚ùå Gap analysis confidence too low: {gap_data.get('confidence', 0):.2f}"
                )

        except Exception as e:
            print(f"‚ö†Ô∏è Gap analysis failed: {e}")
            # Fallback: Erstelle Gap-Analyse basierend auf Query
            if context_quality <= 0.4:
                query_concepts = self._extract_query_concepts(query)
                print(f"üîÑ Fallback gap analysis with concepts: {query_concepts}")

                return DataGapAnalysis(
                    query=query,
                    missing_concepts=query_concepts,
                    confidence_score=0.8,
                    suggested_queries=[],
                    data_sources=["fallback_analysis"],
                    timestamp=datetime.now(),
                )

        return None

    async def generate_missing_knowledge(
        self, gap_analysis: DataGapAnalysis
    ) -> Optional[str]:
        """
        L√§sst die Chat-LLM fehlende Wissensinhalte generieren
        """
        if self.current_imports >= self.max_auto_imports_per_session:
            print(f"üõë Max auto-imports reached ({self.max_auto_imports_per_session})")
            return None

        knowledge_generation_prompt = f"""
        You are an expert knowledge curator. Generate comprehensive, accurate content about the following missing concepts:

        Original Query: {gap_analysis.query}
        Missing Concepts: {", ".join(gap_analysis.missing_concepts)}

        Create a detailed, factual document that covers:
        1. Clear definitions and explanations
        2. Key principles and concepts
        3. Practical applications and examples
        4. Important relationships and connections
        5. Current trends and developments

        Write in a clear, educational style suitable for a knowledge base.
        Focus on accuracy and comprehensiveness.

        Generated Knowledge Base Entry:
        """

        try:
            response = await self.knowledge_generator.agenerate(
                [knowledge_generation_prompt]
            )
            generated_content = response.generations[0][0].text

            if len(generated_content) > 200:  # Mindestl√§nge f√ºr n√ºtzlichen Content
                return generated_content

        except Exception as e:
            print(f"‚ö†Ô∏è Knowledge generation failed: {e}")

        return None

    async def import_generated_knowledge(
        self, content: str, gap_analysis: DataGapAnalysis
    ) -> bool:
        """
        Importiert generierte Wissensinhalte in die Datenbank
        """
        try:
            # Erstelle einzigartigen Dateinamen
            content_hash = hashlib.md5(content.encode()).hexdigest()[:8]
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f"auto_imported_{timestamp}_{content_hash}.txt"

            # Erweitere Content mit Metadaten
            enriched_content = f"""# Auto-Generated Knowledge Base Entry
Generated: {gap_analysis.timestamp}
Source Query: {gap_analysis.query}
Missing Concepts: {", ".join(gap_analysis.missing_concepts)}
Confidence: {gap_analysis.confidence_score:.2f}

---

{content}

---
Auto-imported by Intelligent Data Import Workflow
"""

            # Speichere in Auto-Import-Ordner
            import_file = self.auto_import_path / filename
            import_file.write_text(enriched_content, encoding="utf-8")

            # F√ºge zur Hauptdatenbank hinzu (kopiere in documents)
            main_docs_path = Path(self.rag_system.config.documents_path)
            main_file = main_docs_path / filename
            main_file.write_text(enriched_content, encoding="utf-8")

            # Re-indexiere das RAG-System
            await self._reindex_rag_system()

            self.current_imports += 1
            self.data_import_history.append(gap_analysis)

            print(f"‚úÖ Auto-imported knowledge: {filename}")
            print(f"üìä Concepts added: {', '.join(gap_analysis.missing_concepts)}")

            return True

        except Exception as e:
            print(f"‚ö†Ô∏è Knowledge import failed: {e}")
            return False

    async def _reindex_rag_system(self):
        """
        Re-indexiert das RAG-System mit neuen Dokumenten
        """
        try:
            # Lade alle Dokumente neu
            paths = []
            docs_path = Path(self.rag_system.config.documents_path)
            if docs_path.exists():
                for ext in ("*.txt", "*.md"):
                    paths.extend(list(docs_path.glob(ext)))

            texts = []
            for path in paths:
                try:
                    text = path.read_text(encoding="utf-8", errors="ignore")
                    texts.append(text)
                except Exception:
                    pass

            if texts:
                # Re-indexiere Vector Store
                self.rag_system.vector_store.index_documents(texts)
                print(f"üîÑ Re-indexed {len(texts)} documents")

        except Exception as e:
            print(f"‚ö†Ô∏è Re-indexing failed: {e}")

    async def intelligent_query_with_auto_import(self, query: str) -> Dict[str, Any]:
        """
        Hauptfunktion: Beantwortet Query und importiert automatisch fehlende Daten
        """
        # Erste Query-Ausf√ºhrung
        initial_result = await self.rag_system.query(query)

        # Bewerte Antwortqualit√§t
        context_quality = self._assess_answer_quality(query, initial_result)

        # Analysiere Wissensl√ºcken
        gap_analysis = await self.analyze_knowledge_gaps(
            query, initial_result.get("answer", ""), context_quality
        )

        result = initial_result.copy()
        result["context_quality"] = context_quality
        result["auto_import_triggered"] = False

        if gap_analysis:
            print(
                f"üîç Knowledge gaps detected (confidence: {gap_analysis.confidence_score:.2f})"
            )
            print(f"üìù Missing concepts: {', '.join(gap_analysis.missing_concepts)}")

            # Generiere fehlendes Wissen
            generated_knowledge = await self.generate_missing_knowledge(gap_analysis)

            if generated_knowledge:
                # Importiere neues Wissen
                import_success = await self.import_generated_knowledge(
                    generated_knowledge, gap_analysis
                )

                if import_success:
                    # F√ºhre Query erneut aus mit erweitertem Wissen
                    print("üîÑ Re-running query with enhanced knowledge base...")
                    enhanced_result = await self.rag_system.query(query)

                    result.update(enhanced_result)
                    result["auto_import_triggered"] = True
                    result["imported_concepts"] = gap_analysis.missing_concepts
                    result["knowledge_enhancement"] = {
                        "original_quality": context_quality,
                        "enhanced_sources": enhanced_result.get("sources_count", 0),
                        "improvement_confidence": gap_analysis.confidence_score,
                    }

        return result

    def _assess_answer_quality(self, query: str, result: Dict[str, Any]) -> float:
        """
        Bewertet die Qualit√§t einer Antwort (einfache Heuristik)
        """
        answer = result.get("answer", "")
        sources_count = result.get("sources_count", 0)

        # Basis-Qualit√§tsbewertung
        quality_score = 0.5  # Basis

        # KRITISCH: Keine Quellen = sehr schlechte Qualit√§t
        if sources_count == 0:
            quality_score = 0.1  # Sehr niedrig f√ºr Auto-Import-Trigger
            print(f"‚ö†Ô∏è No sources found - quality reduced to {quality_score}")
        elif sources_count >= 3:
            quality_score += 0.2
        elif sources_count >= 1:
            quality_score += 0.1

        # Antwortl√§nge
        if len(answer) > 200:
            quality_score += 0.1
        if len(answer) > 500:
            quality_score += 0.1

        # Spezifische Indikatoren f√ºr gute Antworten
        if any(
            phrase in answer.lower()
            for phrase in ["based on", "according to", "specifically", "for example"]
        ):
            quality_score += 0.1

        # Indikatoren f√ºr schlechte Antworten - verst√§rkt f√ºr Auto-Import
        bad_indicators = [
            "not enough information",
            "cannot determine",
            "insufficient context",
            "not confidently derivable",
            "more context needed",
            "i don't have enough",
            "no relevant information",
            "cannot find",
            "no information available",
        ]

        if any(phrase in answer.lower() for phrase in bad_indicators):
            quality_score -= 0.4  # St√§rkere Reduktion
            print(
                f"‚ö†Ô∏è Poor answer indicators detected - quality reduced to {quality_score}"
            )

        # Kurze, generische Antworten sind schlecht
        if len(answer) < 100 and sources_count == 0:
            quality_score -= 0.2
            print(
                f"‚ö†Ô∏è Short answer with no sources - quality reduced to {quality_score}"
            )

        final_score = min(max(quality_score, 0.0), 1.0)
        print(
            f"üìä Answer quality assessment: {final_score:.2f} (sources: {sources_count}, answer_length: {len(answer)})"
        )
        return final_score

    def _parse_json_response(self, text: str) -> Dict[str, Any]:
        """
        Parst JSON aus LLM-Antworten (robuste Version)
        """
        try:
            # Versuche direktes JSON-Parsing
            return json.loads(text)
        except BaseException:
            # Extrahiere JSON aus Text
            import re

            json_match = re.search(r"\{.*\}", text, re.DOTALL)
            if json_match:
                try:
                    return json.loads(json_match.group())
                except BaseException:
                    pass

            # Fallback: Dummy-Struktur
            return {
                "has_gaps": "insufficient" in text.lower()
                or "not enough" in text.lower(),
                "missing_concepts": [],
                "confidence": 0.5,
                "suggested_queries": [],
                "data_sources": [],
            }

    async def get_import_statistics(self) -> Dict[str, Any]:
        """
        Statistiken √ºber automatische Importe
        """
        return {
            "total_imports": len(self.data_import_history),
            "current_session_imports": self.current_imports,
            "max_imports_per_session": self.max_auto_imports_per_session,
            "imported_concepts": [
                concept
                for analysis in self.data_import_history
                for concept in analysis.missing_concepts
            ],
            "average_confidence": (
                (
                    sum(a.confidence_score for a in self.data_import_history)
                    / len(self.data_import_history)
                )
                if self.data_import_history
                else 0
            ),
            "auto_import_files": list(self.auto_import_path.glob("*.txt")),
        }

    def _extract_query_concepts(self, query: str) -> List[str]:
        """
        Extrahiert Hauptkonzepte aus einer Query f√ºr Auto-Import
        """
        import re

        # Entferne Stopw√∂rter und extrahiere wichtige Begriffe
        stopwords = {
            "what",
            "how",
            "why",
            "when",
            "where",
            "is",
            "are",
            "the",
            "a",
            "an",
            "and",
            "or",
            "but",
            "in",
            "on",
            "at",
            "to",
            "for",
            "of",
            "with",
            "by",
            "from",
            "up",
            "about",
            "into",
            "through",
            "during",
        }

        # Extrahiere W√∂rter und filtere Stopw√∂rter
        words = re.findall(r"\b[a-zA-Z]{3,}\b", query.lower())
        [word for word in words if word not in stopwords]

        # Kombiniere aufeinanderfolgende W√∂rter zu Konzepten
        combined_concepts = []
        query_parts = query.split()

        for i, part in enumerate(query_parts):
            clean_part = re.sub(r"[^\w\s]", "", part).lower()
            if len(clean_part) > 2 and clean_part not in stopwords:
                # Versuche Mehrwort-Konzepte zu bilden
                if i < len(query_parts) - 1:
                    next_part = re.sub(r"[^\w\s]", "", query_parts[i + 1]).lower()
                    if len(next_part) > 2 and next_part not in stopwords:
                        combined_concepts.append(f"{clean_part} {next_part}")

                combined_concepts.append(clean_part)

        # Entferne Duplikate und begenze auf 5 Konzepte
        unique_concepts = list(dict.fromkeys(combined_concepts))[:5]
        print(f"üìù Extracted concepts from query: {unique_concepts}")

        return unique_concepts


class EnhancedSelfLearningRAG(SelfLearningRAGSystem):
    """
    Erweitert das Self-Learning RAG um automatischen Datenimport
    """

    def __init__(self, base_rag_system, learning_config=None):
        super().__init__(base_rag_system, learning_config)
        self.data_import_workflow = IntelligentDataImportWorkflow(base_rag_system)

    async def intelligent_enhanced_query(
        self, query: str, user_context: Dict = None
    ) -> Dict[str, Any]:
        """
        Kombiniert Self-Learning mit automatischem Datenimport
        """
        # Initialisiere Enterprise-Workarounds falls pending
        if (
            hasattr(self, "_workarounds_pending")
            and self._workarounds_pending
            and WORKAROUNDS_AVAILABLE
        ):
            try:
                print(
                    "üîß Initializing Enterprise Feature Workarounds on first query..."
                )
                enhanced_self = await enhance_rag_with_workarounds(self)

                # √úbertrage alle erweiterten Funktionen
                if hasattr(enhanced_self, "enterprise_analytics"):
                    self.enterprise_analytics = enhanced_self.enterprise_analytics
                if hasattr(enhanced_self, "enterprise_similarity"):
                    self.enterprise_similarity = enhanced_self.enterprise_similarity
                if hasattr(enhanced_self, "enterprise_performance"):
                    self.enterprise_performance = enhanced_self.enterprise_performance
                if hasattr(enhanced_self, "get_analytics_dashboard"):
                    self.get_analytics_dashboard = enhanced_self.get_analytics_dashboard

                print("‚úÖ Enterprise workarounds successfully activated!")
                self._workarounds_pending = False

            except Exception as e:
                print(f"‚ö†Ô∏è Enterprise workarounds initialization failed: {e}")
                self._workarounds_pending = False

        # F√ºhre intelligente Query mit Auto-Import aus
        result = await self.data_import_workflow.intelligent_query_with_auto_import(
            query
        )

        # Wenn Auto-Import stattgefunden hat, verwende Standard-Enhanced-Query
        if result.get("auto_import_triggered", False):
            # F√ºhre Enhanced Query f√ºr Learning-Features aus
            learning_result = await self.enhanced_query(query, user_context)

            # Kombiniere Ergebnisse
            result.update(learning_result)
            result["workflow_type"] = "intelligent_auto_import"
        else:
            # Standard Enhanced Query
            result = await self.enhanced_query(query, user_context)
            result["workflow_type"] = "standard_learning"

        # F√ºge Enterprise Analytics hinzu falls verf√ºgbar
        if hasattr(self, "enterprise_analytics"):
            try:
                # Berechne Node Importance f√ºr relevante Begriffe
                query_concepts = self.data_import_workflow._extract_query_concepts(
                    query
                )
                if query_concepts:
                    node_importance = (
                        await self.enterprise_analytics.calculate_node_importance()
                    )
                    result["enterprise_analytics"] = {
                        "node_importance": len(node_importance),
                        "top_concepts": [n.node_name for n in node_importance[:3]],
                    }
            except Exception as e:
                print(f"‚ö†Ô∏è Enterprise analytics failed: {e}")

        return result


# Factory-Funktion f√ºr einfache Verwendung
def create_intelligent_rag_system(config: RAGConfig = None) -> EnhancedSelfLearningRAG:
    """
    Erstellt ein vollst√§ndig intelligentes RAG-System mit Auto-Import und Enterprise-Workarounds
    """
    if not config:
        config = RAGConfig(
            neo4j_password=None, documents_path="data/documents"
        )  # Lokaler Modus als Standard

    base_rag = AdvancedRAGSystem(config)
    intelligent_rag = EnhancedSelfLearningRAG(base_rag)

    print("üß† Intelligent RAG System with Auto-Import initialized!")
    print("üí° Features: Self-Learning + Automatic Knowledge Gap Filling")

    # Aktiviere Enterprise-Workarounds falls verf√ºgbar
    if WORKAROUNDS_AVAILABLE:
        try:
            print("üîß Initializing Enterprise Feature Workarounds...")
            # Verwende eine separate Initialisierungsfunktion f√ºr
            # async-Operationen
            intelligent_rag._workarounds_pending = True
            print("‚úÖ Enterprise workarounds queued for initialization!")
        except Exception as e:
            print(f"‚ö†Ô∏è Enterprise workarounds failed to queue: {e}")
            print("üìù Continuing with standard RAG features...")
    else:
        print("üìù Enterprise workarounds not available - using standard features")

    return intelligent_rag


async def demo_intelligent_workflow():
    """
    Demonstriert den intelligenten Datenimport-Workflow
    """
    print("üöÄ Starting Intelligent Data Import Workflow Demo")
    print("=" * 60)

    # Erstelle System
    intelligent_rag = create_intelligent_rag_system()

    # Test-Queries die Wissensl√ºcken aufdecken k√∂nnten
    test_queries = [
        "What are the latest developments in quantum machine learning?",
        "How does blockchain technology integrate with renewable energy?",
        "What are the ethical implications of AI in healthcare?",
        "Explain the concept of neuromorphic computing",
    ]

    for i, query in enumerate(test_queries, 1):
        print(f"\nüîç Test Query {i}/4: {query}")

        try:
            result = await intelligent_rag.intelligent_enhanced_query(query)

            print(f"‚úÖ Answer Generated: {len(result.get('answer', ''))} chars")
            print(f"üìä Sources Used: {result.get('sources_count', 0)}")
            print(f"üéØ Quality Score: {result.get('context_quality', 0):.2f}")

            if result.get("auto_import_triggered", False):
                print("üîÑ AUTO-IMPORT TRIGGERED!")
                print(f"üìà New Concepts: {result.get('imported_concepts', [])}")
                enhancement = result.get("knowledge_enhancement", {})
                print(
                    f"üìä Quality Improvement: {enhancement.get('original_quality', 0):.2f} ‚Üí Enhanced"
                )
            else:
                print("‚ÑπÔ∏è No knowledge gaps detected")

        except Exception as e:
            print(f"‚ùå Query failed: {e}")

    # Zeige finale Statistiken
    print("\nüìà Final System Statistics:")
    insights = await intelligent_rag.get_comprehensive_insights()
    auto_stats = insights.get("auto_import_stats", {})

    print(f"   üîÑ Total Auto-Imports: {auto_stats.get('total_imports', 0)}")
    print(
        f"   üìö New Documents Created: {len(auto_stats.get('auto_import_files', []))}"
    )
    print(f"   üß† Concepts Added: {len(set(auto_stats.get('imported_concepts', [])))}")
    print(
        f"   üìä System Evolution: {insights.get('system_evolution', {}).get('adaptive_capability', 'unknown')}"
    )


if __name__ == "__main__":
    asyncio.run(demo_intelligent_workflow())


# Export all public functions
__all__ = [
    "DataGapAnalysis",
    "IntelligentDataImportWorkflow",
    "EnhancedSelfLearningRAG",
    "create_intelligent_rag_system",
    "demo_intelligent_workflow",
]
